{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI4142 - Group 48 - Assignment 2\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "This report details data cleaning and imputation on 2 different datasets.\n",
    "\n",
    "By presenting these findings through a combination of visualizations and actionable insights, this report aims to provide a comprehensive demonstration of data cleaning practices.\n",
    "\n",
    "The target audience for this report includes ____.\n",
    "\n",
    "\n",
    "#### Group 48 Members\n",
    "- Ali Bhangu - 300234254\n",
    "- Justin Wang - 300234186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Python libraries\n",
    "import numpy as npy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing dirty_cafe_sales.csv found. Deleting and re-extracting...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  110k  100  110k    0     0   223k      0 --:--:-- --:--:-- --:--:-- 1844k\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset not found: dirty_cafe_sales.csv. Ensure the file is downloaded and extracted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Check if the CSV file exists, otherwise raise an error\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_path):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Ensure the file is downloaded and extracted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Loading in the dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cafeSet \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found: dirty_cafe_sales.csv. Ensure the file is downloaded and extracted."
     ]
    }
   ],
   "source": [
    "# Checking to see if the csv exists, if so - delete and re-extract\n",
    "csv_path = \"dirty_cafe_sales.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"Existing {csv_path} found. Deleting and re-extracting...\")\n",
    "    os.remove(csv_path)\n",
    "\n",
    "#!/bin/bash\n",
    "!curl -L -o ~/Downloads/cafe-sales-dirty-data-for-cleaning-training.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/ahmedmohamed2003/cafe-sales-dirty-data-for-cleaning-training\n",
    "  \n",
    "# Check if the CSV file exists, otherwise raise an error\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found: {csv_path}. Ensure the file is downloaded and extracted.\")\n",
    "\n",
    "# Loading in the dataset\n",
    "cafeSet = pd.read_csv(csv_path)\n",
    "cafeSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Transaction ID    10000 non-null  object\n",
      " 1   Item              9667 non-null   object\n",
      " 2   Quantity          9862 non-null   object\n",
      " 3   Price Per Unit    9821 non-null   object\n",
      " 4   Total Spent       9827 non-null   object\n",
      " 5   Payment Method    7421 non-null   object\n",
      " 6   Location          6735 non-null   object\n",
      " 7   Transaction Date  9841 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 625.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8807 entries, 0 to 8806\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   show_id       8807 non-null   object\n",
      " 1   type          8807 non-null   object\n",
      " 2   title         8807 non-null   object\n",
      " 3   director      6173 non-null   object\n",
      " 4   cast          7982 non-null   object\n",
      " 5   country       7976 non-null   object\n",
      " 6   date_added    8797 non-null   object\n",
      " 7   release_year  8807 non-null   int64 \n",
      " 8   rating        8803 non-null   object\n",
      " 9   duration      8804 non-null   object\n",
      " 10  listed_in     8807 non-null   object\n",
      " 11  description   8807 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 825.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84548 entries, 0 to 84547\n",
      "Data columns (total 22 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   Unnamed: 0                      84548 non-null  int64 \n",
      " 1   BOROUGH                         84548 non-null  int64 \n",
      " 2   NEIGHBORHOOD                    84548 non-null  object\n",
      " 3   BUILDING CLASS CATEGORY         84548 non-null  object\n",
      " 4   TAX CLASS AT PRESENT            84548 non-null  object\n",
      " 5   BLOCK                           84548 non-null  int64 \n",
      " 6   LOT                             84548 non-null  int64 \n",
      " 7   EASE-MENT                       84548 non-null  object\n",
      " 8   BUILDING CLASS AT PRESENT       84548 non-null  object\n",
      " 9   ADDRESS                         84548 non-null  object\n",
      " 10  APARTMENT NUMBER                84548 non-null  object\n",
      " 11  ZIP CODE                        84548 non-null  int64 \n",
      " 12  RESIDENTIAL UNITS               84548 non-null  int64 \n",
      " 13  COMMERCIAL UNITS                84548 non-null  int64 \n",
      " 14  TOTAL UNITS                     84548 non-null  int64 \n",
      " 15  LAND SQUARE FEET                84548 non-null  object\n",
      " 16  GROSS SQUARE FEET               84548 non-null  object\n",
      " 17  YEAR BUILT                      84548 non-null  int64 \n",
      " 18  TAX CLASS AT TIME OF SALE       84548 non-null  int64 \n",
      " 19  BUILDING CLASS AT TIME OF SALE  84548 non-null  object\n",
      " 20  SALE PRICE                      84548 non-null  object\n",
      " 21  SALE DATE                       84548 non-null  object\n",
      "dtypes: int64(10), object(12)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "cafeSet = pd.read_csv(\"dirty_cafe_sales.csv\")\n",
    "cafeSet.info()\n",
    "\n",
    "\n",
    "# cafeSet = cafeSet.infer_objects()\n",
    "# print(cafeSet.dtypes)\n",
    "\n",
    "# movieSet = pd.read_csv(\"movies.csv\")\n",
    "# movieSet.info()\n",
    "\n",
    "netflixSet = pd.read_csv(\"netflix_titles.csv\")\n",
    "netflixSet.info()\n",
    "\n",
    "newYorkSet = pd.read_csv(\"nyc-rolling-sales.csv\")\n",
    "newYorkSet.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Error\n",
    "\n",
    "A Data Type check ensures that data entered into a field is of the correct data type. A field, for example, may only accept numeric data. The system should then reject any data containing other characters, such as letters or special symbols, and an error message should be displayed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please enter the various attrivutes below to perform the data cleaning process on the dataset. \n",
    "# Input your column \n",
    "testColumn = \"date_added\"\n",
    "expectedType = \"object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "\n",
    "def check_data_type(df, column, expected_type):\n",
    "      # Define a function to check if a value matches the expected type\n",
    "    def is_expected_type(value):\n",
    "        if pd.isna(value):\n",
    "            return False  # Consider NaN as a mismatch\n",
    "        return isinstance(value, expected_type)\n",
    "\n",
    "    # Apply the function to the column and filter rows that don't match the expected type\n",
    "    mismatched_rows = df[~df[column].apply(is_expected_type)]\n",
    "    return mismatched_rows\n",
    "    \n",
    "# Example usage:\n",
    "# Assuming 'netflixSet' is your DataFrame and 'date_added' is the column to check\n",
    "errors = check_data_type(netflixSet, 'date_added', str)\n",
    "if not errors.empty:\n",
    "    print(\"Data type mismatch found in 'date_added' column:\")\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Error\n",
    "A Range Check will determine whether the input data falls within a given range. Latitude and longitude, for example, are frequently used in geographic data. Latitude should be between -90 and 90, and longitude should be between -180 and 180. \n",
    "\n",
    "Any values outside of this range are considered invalid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = []\n",
    "\n",
    "test_attribute = \"\"\n",
    "\n",
    "minimum = 0\n",
    "maximum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_checker(df, column, minimum, maximum):\n",
    "    # Convert column to numeric, coerce errors to NaN\n",
    "    numeric_col = pd.to_numeric(df[column], errors='coerce')\n",
    "    \n",
    "    # Filter directly using the numeric column while ignoring NaNs\n",
    "    below_min = df.loc[(numeric_col < minimum) & numeric_col.notna()]\n",
    "    above_max = df.loc[(numeric_col > maximum) & numeric_col.notna()]\n",
    "    \n",
    "    total_below = below_min.shape[0]\n",
    "    total_above = above_max.shape[0]\n",
    "    \n",
    "    print(f\"There are {total_below} data points with {column} less than {minimum}, and {total_above} data points with {column} over {maximum}.\")\n",
    "    \n",
    "    if total_below > 0:\n",
    "        print(\"\\nExamples below minimum:\")\n",
    "        print(below_min.head(2))  # Show first 2 rows\n",
    "    \n",
    "    if total_above > 0:\n",
    "        print(\"\\nExamples above maximum:\")\n",
    "        print(above_max.head(2))  # Show first 2 rows\n",
    "\n",
    "# Example usage:\n",
    "range_checker(cafeSet, 'Price Per Unit', 3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Errors\n",
    "\n",
    "Many data types have a predefined format. A Format Check will ensure that the data is in the correct format. Date fields, for example, are stored in a fixed format such as “YYYY-MM-DD” or “DD-MM-YYYY.” If the date is entered in any other format, it will be rejected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please enter the various attrivutes below to perform the data cleaning process on the dataset. \n",
    "# Input your column \n",
    "column = \"Transaction Date\"\n",
    "pattern = r'^\\d{4}-\\d{2}-\\d{2}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 460 data points in Transaction Date that do not match the format ^\\d{4}-\\d{2}-\\d{2}$.\n",
      "\n",
      "Examples of mismatched format:\n",
      "   Transaction ID      Item Quantity Price Per Unit Total Spent  \\\n",
      "11    TXN_3051279  Sandwich        2            4.0         8.0   \n",
      "29    TXN_7640952      Cake        4            3.0        12.0   \n",
      "\n",
      "    Payment Method  Location Transaction Date  \n",
      "11     Credit Card  Takeaway            ERROR  \n",
      "29  Digital Wallet  Takeaway            ERROR  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_checker(df, column, pattern):\n",
    "    # Compile the regex pattern\n",
    "    regex = re.compile(pattern)\n",
    "    \n",
    "    # Apply the regex pattern to the column and filter rows that don't match\n",
    "    mismatched_rows = df[~df[column].astype(str).apply(lambda x: bool(regex.match(x)))]\n",
    "    \n",
    "    total_mismatched = mismatched_rows.shape[0]\n",
    "    \n",
    "    print(f\"There are {total_mismatched} data points in {column} that do not match the format {pattern}.\")\n",
    "    \n",
    "    if total_mismatched > 0:\n",
    "        print(\"\\nExamples of mismatched format:\")\n",
    "        print(mismatched_rows.head(2))  # Show first 2 rows\n",
    "\n",
    "# Example usage:\n",
    "format_checker(cafeSet, column, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency Errors\n",
    "\n",
    "A Consistency Check is a type of logical check that ensures data is entered in a logically consistent manner. Checking if the delivery date for a parcel is after the shipping date is one example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please enter the various attrivutes below to perform the data cleaning process on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1456 consistency errors in 'Total Spent'.\n",
      "   Transaction ID  Price Per Unit  Quantity  Total Spent  expected_total\n",
      "2     TXN_4271903             1.0       4.0          NaN             4.0\n",
      "20    TXN_3522028             4.0       NaN         20.0             NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pretty hard coded, could be improved with generalization\n",
    "\n",
    "def consistency_checker(df):\n",
    "    \"\"\"\n",
    "    Checks for consistency between Total Spent and Price Per Unit * Quantity.\n",
    "    \"\"\"\n",
    "    # Convert to numeric, ignoring non-numeric values\n",
    "    df['Price Per Unit'] = pd.to_numeric(df['Price Per Unit'], errors='coerce')\n",
    "    df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
    "    df['Total Spent'] = pd.to_numeric(df['Total Spent'], errors='coerce')\n",
    "    \n",
    "    # Calculate expected total\n",
    "    df['expected_total'] = df['Price Per Unit'] * df['Quantity']\n",
    "    \n",
    "    # Find inconsistencies where Total Spent doesn't match expected total\n",
    "    inconsistent = df[df['Total Spent'] != df['expected_total']]\n",
    "    \n",
    "    if inconsistent.empty:\n",
    "        print(\"No consistency errors found.\")\n",
    "    else:\n",
    "        print(f\"Found {len(inconsistent)} consistency errors in 'Total Spent'.\")\n",
    "        print(inconsistent[['Transaction ID', 'Price Per Unit', 'Quantity', 'Total Spent', 'expected_total']].head(2))\n",
    "    \n",
    "    return inconsistent\n",
    "\n",
    "# Example usage:\n",
    "inconsistencies = consistency_checker(cafeSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness Errors\n",
    "\n",
    "Some data, such as IDs or e-mail addresses, are inherently unique. These fields in a database should most likely have unique entries. A Uniqueness Check ensures that an item is not entered into a database more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testColumn = 'Transaction ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 duplicate entries in 'Transaction ID'.\n",
      "  Transaction ID\n",
      "0    TXN_1961373\n",
      "1    TXN_1961373\n"
     ]
    }
   ],
   "source": [
    "# add user input firleds above ^ \n",
    "def uniqueness_checker(df, column):\n",
    "    \"\"\"\n",
    "    Checks for uniqueness errors in a specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "    column : str, column name to check for unique values.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with duplicate rows.\n",
    "    \"\"\"\n",
    "    duplicates = df[df.duplicated(subset=[column], keep=False)]\n",
    "    \n",
    "    if duplicates.empty:\n",
    "        print(f\"All values in '{column}' are unique.\")\n",
    "    else:\n",
    "        print(f\"Found {len(duplicates)} duplicate entries in '{column}'.\")\n",
    "        print(duplicates[[column]].head(2))  # Display first 2 duplicates\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Example usage:\n",
    "duplicates = uniqueness_checker(cafeSet, testColumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence Errors\n",
    "\n",
    "A Presence Check ensures that all mandatory fields are not left blank. If someone tries to leave the field blank, an error message will be displayed, and they will be unable to proceed to the next step or save any other data that they have entered. A key field, for example, cannot be left blank in most databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "testColumn = 'Item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 677 missing values in 'Item'.\n",
      "       Item\n",
      "6   UNKNOWN\n",
      "8       NaN\n",
      "30      NaN\n",
      "31  UNKNOWN\n",
      "33  UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "# user input this shit \n",
    "\n",
    "def presence_checker(df, column):\n",
    "    \"\"\"\n",
    "    Checks for missing values in a specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "    column : str, column name to check for missing values.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with rows where the specified column is missing.\n",
    "    \"\"\"\n",
    "    missing = df[df[column].isna() | (df[column].str.lower() == 'unknown')]\n",
    "    \n",
    "    if missing.empty:\n",
    "        print(f\"No missing values found in '{column}'.\")\n",
    "    else:\n",
    "        print(f\"Found {len(missing)} missing values in '{column}'.\")\n",
    "        print(missing[[column]].head())  # Display first 2 rows with missing values\n",
    "    \n",
    "    return missing\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "missing_values = presence_checker(cafeSet, testColumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Errors\n",
    "\n",
    "A Length Check ensures that the appropriate number of characters are entered into the field. It verifies that the entered character string is neither too short nor too long. Consider a password that must be at least 8 characters long. The Length Check ensures that the field is filled with exactly 8 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "testColumn = 'Price Per Unit'\n",
    "testLength = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 354 entries in 'Price Per Unit' that do not meet the length requirement of 3.\n",
      "    Price Per Unit\n",
      "68           ERROR\n",
      "140        UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "#again above \n",
    "\n",
    "def length_checker(df, column, length):\n",
    "    \"\"\"\n",
    "    Checks for length errors in a specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "    column : str, column name to check.\n",
    "    length : int, required length for each value.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with rows where the specified column does not meet the length requirement.\n",
    "    \"\"\"\n",
    "    # Convert column to string and check length\n",
    "    invalid_length = df[df[column].astype(str).str.len() != length]\n",
    "    \n",
    "    if invalid_length.empty:\n",
    "        print(f\"All values in '{column}' meet the length requirement of {length}.\")\n",
    "    else:\n",
    "        print(f\"Found {len(invalid_length)} entries in '{column}' that do not meet the length requirement of {length}.\")\n",
    "        print(invalid_length[[column]].head(2))  # Display first 2 invalid entries\n",
    "    \n",
    "    return invalid_length\n",
    "\n",
    "# Example usage:\n",
    "invalid_length = length_checker(cafeSet, testColumn, testLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look-Up Errors\n",
    "\n",
    "Look Up assists in reducing errors in a field with a limited set of values. It consults a table to find acceptable values. The fact that there are only 7 possible days in a week, for example, ensures that the list of possible values is limited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "testColumn = 'Payment Method'\n",
    "valid_payment_methods = [\"Credit Card\", \"Cash\", \"Digital Wallet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3178 invalid entries in 'Payment Method'.\n",
      "  Payment Method\n",
      "3        UNKNOWN\n",
      "6          ERROR\n"
     ]
    }
   ],
   "source": [
    "def lookup_checker(df, column, valid_values):\n",
    "    \"\"\"\n",
    "    Checks for look-up errors in a specified column based on a list of valid values.\n",
    "    \n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "    column : str, column name to check.\n",
    "    valid_values : list, list of expected values.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with rows where the column contains invalid values.\n",
    "    \"\"\"\n",
    "    invalid_values = df[~df[column].isin(valid_values)]\n",
    "    \n",
    "    if invalid_values.empty:\n",
    "        print(f\"All values in '{column}' are valid.\")\n",
    "    else:\n",
    "        print(f\"Found {len(invalid_values)} invalid entries in '{column}'.\")\n",
    "        print(invalid_values[[column]].head(2))  # Display first 2 invalid entries\n",
    "    \n",
    "    return invalid_values\n",
    "\n",
    "\n",
    "# Check Payment Method in cafeSet\n",
    "lookup_errors = lookup_checker(cafeSet, testColumn, valid_payment_methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Duplicate Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Near Duplicate Errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
